<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-10-09T21:21:43-05:00</updated><id>http://localhost:4000/</id><title type="html">Yet Another Machine Learning Blog</title><subtitle>I am starting this blog to offer my take on the statistical, and algorithmic aspects of Machine Learning.
</subtitle><author><name>praneeth narayanamurthy</name></author><entry><title type="html">Central Limit Theorem and Weak Law of Large Numbers</title><link href="http://localhost:4000/2018/09/24/central-limit.html" rel="alternate" type="text/html" title="Central Limit Theorem and Weak Law of Large Numbers" /><published>2018-09-24T22:45:00-05:00</published><updated>2018-09-24T22:45:00-05:00</updated><id>http://localhost:4000/2018/09/24/central-limit</id><content type="html" xml:base="http://localhost:4000/2018/09/24/central-limit.html">&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;After a (&lt;em&gt;really&lt;/em&gt;) long hiatus, I have resolved to start writing these posts more freqently. The first actual post I attempted (which is still incomplete) turned out to be a lot more effort than I initially fathomed. Thus, in this post, I decided to set more realistic goals. At the outset, I do not claim any of the below material to be my own, but rather the culmination of effort to &lt;em&gt;restart&lt;/em&gt; the habit of reading and writing more frequently. Secondly, although the title of this post is a comprehensive topic (indeed, there is a mind-boggling number of books written on the subject, also feel free to check out the &lt;a href=&quot;https://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/&quot;&gt;excellent post by Terence Tao&lt;/a&gt;) I will focus on two specific aspects. (i) I will introduce the Central Limit Theorem (CLT) and the Weak Law of Large Numbers (WLLN), and prove these results using the idea of Characteristic Functions; and (ii) I will provide a numerical experiment that shows the CLT in action.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Central Limit Theorem and Weak Law of Large Numbers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Many of you might have noticed that most Machine Learning theoreticians (definitely in most engineering fields) often assume that the underlying probability distribution for &lt;em&gt;any&lt;/em&gt; problem make the assumption of Gaussianity, i.e., they assume that the true/natural data obeys a Gaussian distribution. This may seem contrived at the first glance because: why does nature have to obey Gaussian distribution? Well, a part of the justification might arise from the implications of the Central Limit Theorem. Said simply, it states that if you draw &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; independent, and identically distributed (i.i.d.) samples, &lt;script type=&quot;math/tex&quot;&gt;X_1, X_2, \cdots, X_n&lt;/script&gt; from &lt;em&gt;any&lt;/em&gt; probability distribtion &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; with mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, then CLT states the following: 
\begin{equation}
\sqrt{n}\left(\frac{X_1 + X_2 + \cdots + X_n}{n} - \mu\right) \overset{d}{\to} \mathcal{N}(0, \sigma^2)
\end{equation}
where the symbol &lt;script type=&quot;math/tex&quot;&gt;\overset{d}{\to}&lt;/script&gt; denotes that the random variable on the left “converges in distribution” to the distribution on the right as &lt;script type=&quot;math/tex&quot;&gt;n \to \infty&lt;/script&gt;. Qualitatively, what that means is that if one were to draw multiple copies of the r.v. on the left and “plot their histrogram”, it would look more and more like a Gaussian density as you increase the value of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The next result is the Weak Law of Large numbers. Consider, the same set of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; i.i.d. samples from the same distribution &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; as before. WLLS states that 
\begin{equation}
\frac{1}{n}(X_1 + X_2 + \cdots + X_n) \overset{p}{\to} \mu 
\end{equation}
where the symbol &lt;script type=&quot;math/tex&quot;&gt;\overset{p}{\to}&lt;/script&gt; means that the r.v. on the left “converges in probability” to the value on the right as &lt;script type=&quot;math/tex&quot;&gt;n \to \infty&lt;/script&gt;. Qualitatively, this means that if you give my &lt;em&gt;any&lt;/em&gt; deviation, &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; (from the mean) I can give you a large enough value of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; such that with probability &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; the r.v. belongs to the interval &lt;script type=&quot;math/tex&quot;&gt;[\mu-\epsilon, \mu + \epsilon]&lt;/script&gt;. The more precise statment for the above is
\begin{equation}
\lim_{n \to \infty} \mathbb{P}\left( \left|\frac{1}{n}(X_1 + X_2 + \cdots + X_n) - \mu \right| &amp;gt; \epsilon\right) = 0
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Central Limit Theorem:&lt;/strong&gt;
There are multiple ways in which one can prove the CLT but I will focus on using the Characteristic Function (CF) (since I major in Signal Processing) approach. Recall that the CFof a r.v. &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is the expectation of the Fourier Transform, i.e., &lt;script type=&quot;math/tex&quot;&gt;\psi_X(t) = \mathbb{E}[e^{itX}]&lt;/script&gt;. We will use these important properties of the CF (i) for independent r.v.’s &lt;script type=&quot;math/tex&quot;&gt;X,Y&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\psi_{X+Y}(t) = \psi_X(t) \psi_Y(t)&lt;/script&gt;; (ii) &lt;script type=&quot;math/tex&quot;&gt;\psi_{aX}(t) = \psi_{X}(at)&lt;/script&gt; for a positive scalar &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;; and (ii) the CF completely defines a r.v., and that the CF of a Gaussian r.v. is the density itself. 
Now, we expand the l.h.s. of the CLT equation and define the following r.v.’s to make the ideas easy to follow. 
\begin{equation}
\sqrt{n}\left(\frac{X_1 + X_2 + \cdots + X_n}{n} - \mu\right) = \sqrt{n} \cdot \frac{\sum_{i=1}^n (X_i - \mu)}{n} =  \frac{\sum_{i=1}^n (X_i - \mu)}{\sqrt{n}} 
\end{equation}
Also, notice that if we let &lt;script type=&quot;math/tex&quot;&gt;Z_i = \frac{X_i - \mu}{\sigma}&lt;/script&gt;, the r.v.’s &lt;script type=&quot;math/tex&quot;&gt;Z_i&lt;/script&gt; are zero-mean, unit variance and still i.i.d. Next define &lt;script type=&quot;math/tex&quot;&gt;Y_n = \frac{1}{\sqrt{n}}\sum_{i=1}^n Z_i&lt;/script&gt; and consider its CF. Since &lt;script type=&quot;math/tex&quot;&gt;Z_i&lt;/script&gt; are i.i.d., we will drop the subscript and use &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; to mean any of the identical copies
\begin{align}
\psi_{Y_n}(t) &amp;amp;= \psi_{Z_1}\left(\frac{t}{\sqrt{n}}\right) \cdot \psi_{Z_2}\left(\frac{t}{\sqrt{n}}\right) \cdots \psi_{Z_n}\left(\frac{t}{\sqrt{n}}\right)  \\
&amp;amp;= \left(\psi_Z\left(\frac{t}{\sqrt{n}}\right)\right)^n =  \left(\mathbb{E}\left[e^{\frac{itZ}{\sqrt{n}}} \right]\right)^n
\end{align}
now, from the Taylor series expansion of &lt;script type=&quot;math/tex&quot;&gt;e^x&lt;/script&gt;, it is easy to see that 
\begin{align}
\mathbb{E}\left[e^{\frac{itZ}{\sqrt{n}}} \right] &amp;amp;= \mathbb{E}\left[1 + \frac{itZ}{\sqrt{n}} -  \frac{t^2 Z^2}{2n} -  \frac{it^3 Z^3}{3! n^{3/2}} + \cdots \right] \\
&amp;amp;= 1 + 0 -  \frac{t^2}{2n} -  \frac{it^3 \mathbb{E}[Z^3]}{3! n^{3/2}} + \cdots 
\end{align} 
the important point to observe here is that all higher powers (&lt;script type=&quot;math/tex&quot;&gt;\geq 3&lt;/script&gt;) become infinitesimally small as &lt;script type=&quot;math/tex&quot;&gt;n \to \infty&lt;/script&gt; and thus
\begin{equation}
\psi_Z(t) = 1 - \frac{t^2}{2n} + r\left(\frac{t}{\sqrt{n}}\right)
\end{equation}
where &lt;script type=&quot;math/tex&quot;&gt;r(\cdot)&lt;/script&gt; is the “infinitesimal residual”. Now, pluggin that value into the CF of &lt;script type=&quot;math/tex&quot;&gt;Y_n&lt;/script&gt;, we obtain
\begin{align}
\psi_{Y_n}(t) = \left(1 -\frac{t^2}{2n} + r\left(\frac{t}{\sqrt{n}}\right)  \right)^n =  \left(\left(1 -\frac{t^2}{2n} + r\left(\frac{t}{\sqrt{n}}\right)  \right)^{-2n/t^2}\right) ^{\frac{-t^2}{2}} \to e^{\frac{-t^2}{2}}
\end{align}
as &lt;script type=&quot;math/tex&quot;&gt;n \to \infty&lt;/script&gt;. This proves that the distribution of  &lt;script type=&quot;math/tex&quot;&gt;Y_n&lt;/script&gt; tends to become closer the the standard Gaussian distribution with increasing values of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. Finally, applying a rescaling of &lt;script type=&quot;math/tex&quot;&gt;Y_n \to \sigma Y_n&lt;/script&gt;, we obtain the simplification of the l.h.s. of the main theorem. This proves CLT.  &lt;script type=&quot;math/tex&quot;&gt;\boxtimes&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Weak Law of Large Numbers:&lt;/strong&gt;
Again, here there are multiple methods to prove the WLLN but I will focus on using CF. Again, since the r.v.’s &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; are i.i.d., we will often drop the subscript and use &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; to denote any of the identical copies. First, we define &lt;script type=&quot;math/tex&quot;&gt;Y_n = \frac{\sum_{i=1}^n X_1 + X_2 + \cdots + X_n}{n}&lt;/script&gt; and now consider the CF of &lt;script type=&quot;math/tex&quot;&gt;Y_n&lt;/script&gt;. Recalling the two basic properties of CF from the previous section we obtain&lt;br /&gt;
\begin{align}
\psi_{Y_n}(t) &amp;amp;= \psi_{X_1/n}(t) \cdot \psi_{X_2/n}(t) \cdots \psi_{X_n/n}(t) \\
&amp;amp;= (\psi_{X/n}(t))^n = \left(\psi_X \left(\frac{t}{n}\right) \right)^n
\end{align}
now, using the same ideas as before: defintion of CF, linearity of expectation, and the Taylor series expansion of &lt;script type=&quot;math/tex&quot;&gt;e^x&lt;/script&gt;, it follows that
\begin{align}
\psi_{Y_n}(t) &amp;amp;= \left( 1 +  \frac{it\mu}{n} - \frac{t^2 \mathbb{E}[X^2]}{2! n^2} - \frac{it^3 \mathbb{E}[X^3]}{3! n^3} +  \cdots \right)^n \\
&amp;amp;= \left( 1 +  \frac{it\mu}{n} +  r\left(\frac{t}{n}\right) \right)^n \\
&amp;amp;= \left( \left( 1 +  \frac{it\mu}{n} +  r\left(\frac{t}{n}\right) \right)^{\frac{n}{it \mu}}\right)^{i t \mu} \to e^{it\mu} \ \  \text{as} \ \  n \to \infty
\end{align}
and the “residual”, &lt;script type=&quot;math/tex&quot;&gt;r(t/n) \to 0&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;n \to \infty&lt;/script&gt;. Finally, recall that due to the fact that the CF is the Fourier Transform of the density function, the Delta dirac, at &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; has the CF &lt;script type=&quot;math/tex&quot;&gt;e^{it\mu}&lt;/script&gt; which means that &lt;script type=&quot;math/tex&quot;&gt;Y_n \overset{d}{\to} \mu&lt;/script&gt; and since &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is a constant, it implies that &lt;script type=&quot;math/tex&quot;&gt;Y_n \overset{p}{\to} \mu&lt;/script&gt; which proves WLLN. &lt;script type=&quot;math/tex&quot;&gt;\boxtimes&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Numerical Verification of Central Limit Theorem:&lt;/strong&gt;
Here, I show using a simple numerical example to show CLT in practice. The exercise I performed here is the following. I generated &lt;script type=&quot;math/tex&quot;&gt;n \approx 45000&lt;/script&gt; samples (&lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;) from the uniform random distribution, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{U}(0,1)&lt;/script&gt; which denotes that all entries are between &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. The mean is &lt;script type=&quot;math/tex&quot;&gt;\mu = 0.5&lt;/script&gt; and the variance is &lt;script type=&quot;math/tex&quot;&gt;\sigma^2 = 1/12&lt;/script&gt; for this particular distribution. Next, for varying values of &lt;script type=&quot;math/tex&quot;&gt;N \in [10, n]&lt;/script&gt;, I computed the “empirical density” of &lt;script type=&quot;math/tex&quot;&gt;\sqrt{N} \left(\frac{\sum_{i} X_i}{N} - \mu\right)&lt;/script&gt;. I also plotted the “true density”, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(0, \sigma^2)&lt;/script&gt;. It is very interesting to see that as &lt;script type=&quot;math/tex&quot;&gt;N \to n&lt;/script&gt; (since we cannot really use the upper bound of &lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;) the histogram converges very closely to match the Gaussian density, thus confirming claim of CLT. Furthermore, notice that here I show the figures at a logarithmic scale in &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; and in some sense, the empirical histogram converges “linearly” to the true density. I might make a another post detailing the convergence rates of CLT in the near future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/BMIl5HB2ukMnCCchUw/giphy.gif&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is the code snippet used to create the above animation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env/ python&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#import libraries&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;#generate samples using uniform random distribution&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;nrange&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Nrange&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Nrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;skyblue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N = {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And, that’s all folks!!&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = praneethmurthy.github.io;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = ; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://praneethmurthy-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</content><author><name>praneeth narayanamurthy</name></author><category term="probability," /><category term="characterisitic" /><category term="function," /><category term="central" /><category term="limit" /><category term="theorem" /><summary type="html">Hello!</summary></entry><entry><title type="html">Dimensionality Reduction</title><link href="http://localhost:4000/2017/12/29/dimensionality-reduction.html" rel="alternate" type="text/html" title="Dimensionality Reduction" /><published>2017-12-29T12:25:00-06:00</published><updated>2017-12-29T12:25:00-06:00</updated><id>http://localhost:4000/2017/12/29/dimensionality-reduction</id><content type="html" xml:base="http://localhost:4000/2017/12/29/dimensionality-reduction.html">&lt;p&gt;The machine learning community has experimented with various “pre-processing” steps before performing any analysis on data. Two natural techniques (assuming that the data is “clean” and “usable by a program”) are (i) Blowing up the dimensionality of data and (ii) reducing the dimension of the data. Both these approaches are known to perform exceptionally well in certain circumstances. For example, what Support Vector Machines essentially do is blow up the dimensionality of the data (and if the kernel used is a Gaussian kernel, the dimension is actually increased to infinity!) followed by learning a decition boundary in the “lifted space” whereas, for example in the k Nearest Neighbour problem, in most applications the first step involves a dimensionality reduction. In this article I will explain some of the intuition of two very popular methods for dimensionality reduction: Principal Components’ Analysis and Random Projections.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = praneethmurthy.github.io;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = ; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://praneethmurthy-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</content><author><name>praneeth narayanamurthy</name></author><category term="pca," /><category term="svd," /><category term="johnson," /><category term="lindenstrauss" /><summary type="html">The machine learning community has experimented with various “pre-processing” steps before performing any analysis on data. Two natural techniques (assuming that the data is “clean” and “usable by a program”) are (i) Blowing up the dimensionality of data and (ii) reducing the dimension of the data. Both these approaches are known to perform exceptionally well in certain circumstances. For example, what Support Vector Machines essentially do is blow up the dimensionality of the data (and if the kernel used is a Gaussian kernel, the dimension is actually increased to infinity!) followed by learning a decition boundary in the “lifted space” whereas, for example in the k Nearest Neighbour problem, in most applications the first step involves a dimensionality reduction. In this article I will explain some of the intuition of two very popular methods for dimensionality reduction: Principal Components’ Analysis and Random Projections.</summary></entry><entry><title type="html">Hello World Post</title><link href="http://localhost:4000/2017/08/16/hello-world.html" rel="alternate" type="text/html" title="Hello World Post" /><published>2017-08-16T00:41:00-05:00</published><updated>2017-08-16T00:41:00-05:00</updated><id>http://localhost:4000/2017/08/16/hello-world</id><content type="html" xml:base="http://localhost:4000/2017/08/16/hello-world.html">&lt;p&gt;Hello everyone. Welcome to YAMLB!! In this series of (very) ambitious blog posts, I intend to put forth my take on the theoretical, and algorithmic aspects of Machine Learning. According to the &lt;em&gt;Economist.com&lt;/em&gt;, &lt;a href=&quot;https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/&quot;&gt;Data is the new oil&lt;/a&gt;. Although I am not entirely convinced of the validiy of the metaphor, what is true is that the utility of Data, and hence Data Science, has experienced an unprecedented growth in the last two decades. Scientists from various backgrounds are coming together in collecting, cleaning, and making sense of data. All these broadly fall under the purview of &lt;em&gt;Big Data&lt;/em&gt; or &lt;em&gt;Machine Learning&lt;/em&gt; if you will. As of the current post date, I will mainly focus on the last task, i.e., making sense of data.&lt;/p&gt;

&lt;p&gt;I hope that this blog helps someone (myself included!) find answers or maybe even better, a question or two worth pondering over.&lt;/p&gt;

&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;
&lt;script&gt;

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = praneethmurthy.github.io;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = ; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://praneethmurthy-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</content><author><name>praneeth narayanamurthy</name></author><category term="hello," /><category term="world" /><summary type="html">Hello everyone. Welcome to YAMLB!! In this series of (very) ambitious blog posts, I intend to put forth my take on the theoretical, and algorithmic aspects of Machine Learning. According to the Economist.com, Data is the new oil. Although I am not entirely convinced of the validiy of the metaphor, what is true is that the utility of Data, and hence Data Science, has experienced an unprecedented growth in the last two decades. Scientists from various backgrounds are coming together in collecting, cleaning, and making sense of data. All these broadly fall under the purview of Big Data or Machine Learning if you will. As of the current post date, I will mainly focus on the last task, i.e., making sense of data.</summary></entry></feed>