---
title: Multi-Armed Bandit Reading Group
layout: page
tags: Online learning, Reinforcement learning, bandits
permalink: /notes/Bandits.html
---

In July 2020, along with my advisor, I initiated a reading group to understand the area of Multi-Armed Bandits. The discussions are largely adapted from the recently online book on [Bandit Algorithms](https://tor-lattimore.com/downloads/book/book.pdf). Written notes for the meetings are provided below. 

* [Lecture 1:](/assets/bandit_1.pdf "Notes") Introduction to stochastic multi-armed (finite) bandits, explore-then-commit, UCB.

* [Lecture 2:](/assets/bandit_2.pdf "Notes") Asymptotic optimality of UCB, MOSS, Adversarial bandit, and Exp3 algorithm.

* [Lecture 3:](/assets/bandit_3.pdf "Notes") Exp3-IX algorithm.

* [Lecture 4:](/assets/bandit_4.pdf "Notes") Contextual bandits, bandits with expert advice, Exp4 algorithm.

* [Lecture 5:](/assets/bandit_5.pdf "Notes") Stochastic Linear bandits, LinUCB.

* [Lecture 6:](https://github.com/praneethmurthy/praneethmurthy.github.io/blob/master/assets/bandits_examples.ipynb "Notes") Notebook explaining algorithms (under prep)

* [Lecture 7:](/assets/bandit_pca_notes.pdf "Notes") Bandit PCA.

