---
title: Multi-Arm Bandit Reading Group
layout: page
tags: Online learning, Reinforcement learning, bandits
permalink: /notes/Bandits.html
---

* [Lecture 1:](/assets/bandit_1.pdf "Notes") Introduction to stochastic multi-armed (finite) bandits, explore-then-commit, UCB.

* [Lecture 2:](/assets/bandit_2.pdf "Notes") Asymptotic optimality of UCB, MOSS, Adversarial bandit, and Exp3 algorithm.