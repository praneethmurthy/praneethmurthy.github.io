---
title: Multi-Arm Bandit Reading Group
layout: page
tags: Online learning, Reinforcement learning, bandits
permalink: /notes/Bandits.html
---

In July 2020, along with my advisor, I initiated a reading group to understand the area of Multi-Arm Bandits. The discussions are largely adapted from the recently online book on [Bandit Algorithms](https://tor-lattimore.com/downloads/book/book.pdf). Written notes for the meetings are provided below. 

* [Lecture 1:](/assets/bandit_1.pdf "Notes") Introduction to stochastic multi-armed (finite) bandits, explore-then-commit, UCB.

* [Lecture 2:](/assets/bandit_2.pdf "Notes") Asymptotic optimality of UCB, MOSS, Adversarial bandit, and Exp3 algorithm.

* [Lecture 3:](/assets/bandit_3.pdf "Notes") Exp3-IX algorithm.